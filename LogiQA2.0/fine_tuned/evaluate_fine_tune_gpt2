import sys
import os
import json
from typing import List, Dict
import torch
from torch.utils.data import DataLoader, Dataset
from transformer_lens import HookedTransformer
import matplotlib.pyplot as plt

sys.path.append("/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI")
os.chdir("/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI")

# Load the LogiQA2.0 test dataset
with open("test.txt", "r") as file:
    test_data = [json.loads(line) for line in file]

# Remove indices that are too long
indices_to_remove = [1058, 778, 399, 20, 117, 211, 288, 434, 581, 88, 89]
test_data = [item for i, item in enumerate(test_data) if i not in indices_to_remove]

# Prepare the dataset for testing
def prepare_data(data: List[Dict]) -> List[Dict]:
    prepared_data = []
    for item in data:
        label = item["label"]
        major_premise = " ".join(item["major_premise"])
        conclusion = item["conclusion"]
        minor_premise = item["minor_premise"]
        
        # Concatenate major premise, minor premise, and conclusion
        input_text = f"Major Premise: {major_premise}\nMinor Premise: {minor_premise}\nConclusion: {conclusion}"
        
        prepared_data.append({"input_text": input_text, "label": label})
    
    return prepared_data

test_dataset = prepare_data(test_data)

# Create a custom dataset class
class LogiQADataset(Dataset):
    def __init__(self, data):
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        input_text = item["input_text"]
        label = item["label"]
        return input_text, label

# Create a DataLoader
test_dataset = LogiQADataset(test_dataset)
test_loader = DataLoader(test_dataset, batch_size=4)

# Load the fine-tuned model
model = HookedTransformer.from_pretrained("finetuned_gpt2_logiqa")

# Evaluate the model on the test set
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

losses = []

with torch.no_grad():
    for batch in test_loader:
        input_texts, labels = batch
        input_texts = [model.tokenizer.encode(text) for text in input_texts]
        input_ids = torch.tensor(input_texts).to(device)
        
        outputs = model(input_ids)
        logits = outputs[:, -1, :]
        
        # Compute the loss
        # (You'll need to define the loss function based on your specific task)
        # loss = ...
        
        losses.append(loss.item())

# Plot the loss
plt.figure(figsize=(10, 5))
plt.plot(losses)
plt.xlabel("Batch")
plt.ylabel("Loss")
plt.title("Test Loss")
plt.show()