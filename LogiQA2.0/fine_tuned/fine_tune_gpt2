import sys
import os
import json
from typing import List, Dict
from transformer_lens import HookedTransformer
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from torch.nn.utils.rnn import pad_sequence
from torch.optim import AdamW
import torch.nn as nn
from torch.nn import CrossEntropyLoss
from transformers import GPT2Tokenizer

sys.path.append("/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI")
os.chdir("/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI")

# Load the LogiQA2.0 train dataset
with open("train_handcraft.txt", "r") as file:
    train_data = [json.loads(line) for line in file]

# Remove indices that are too long
train_data = [item for i, item in enumerate(train_data)]

# Load the pre-trained GPT-2 model
model = HookedTransformer.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# class GPT2ForBinaryClassification(nn.Module):
#     def __init__(self, base_model, num_labels=2):
#         super(GPT2ForBinaryClassification, self).__init__()
#         self.base_model = base_model  # This is your HookedTransformer model
#         # Add a linear layer for binary classification
#         mode_cfg = self.base_model.cfg
#         #print("model config: ", model_cfg)
#         embedding_dim = model_cfg.d_model
#         #print("embedding_dim", embedding_dim)
#         self.classifier = nn.Linear(embedding_dim, num_labels)

#     def forward(self, input_ids, attention_mask=None):
#         # Get the outputs from the base model
#         #outputs = self.base_model(input_ids, attention_mask=attention_mask).to(device)
#         # Usually, we take the output associated with the first token for classification
#         last_hidden_state = self.base_model(input_ids, return_type=None, stop_at_layer=-1).to(device)
#         sequence_output = last_hidden_state[:, 0, :]
#         output = self.classifier(sequence_output)
#         return output


# Prepare the dataset for training
def prepare_data(data):
    tokenized_data = []
    for item in data:
        label = item["label"]
        major_premise = " ".join(item["major_premise"])
        conclusion = item["conclusion"]
        minor_premise = item["minor_premise"]
        input_text = f"Major Premise: {major_premise}\nMinor Premise: {minor_premise}\nConclusion: {conclusion}"
        # Tokenize input_text here with padding and truncation
        tokenized_inputs = tokenizer(input_text, 
                                     padding='max_length', 
                                     truncation=True, 
                                     max_length=1024, 
                                     return_tensors="pt")
        tokenized_data.append({"input_ids": tokenized_inputs["input_ids"].squeeze(0),  # Remove batch dimension
                               "attention_mask": tokenized_inputs["attention_mask"].squeeze(0), 
                               "labels": label})
    return tokenized_data

train_dataset = prepare_data(train_data)


# Create a custom dataset class
class LogiQADataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        input_ids = item["input_ids"]
        attention_mask = item["attention_mask"]
        label = item["labels"]
        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": label}

# Create a DataLoader
train_dataset = LogiQADataset(train_dataset)
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)

# Load the pre-trained GPT-2 model
base_model = HookedTransformer.from_pretrained("gpt2")
model = base_model.cfg  # Assuming cfg is the instance of HookedTransformerConfig within your HookedTransformer instance
#model = GPT2ForBinaryClassification(base_model=base_model)

# Fine-tune the model
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
model = model.to(device)
model.train()

#define hhyperparameters
optimizer = AdamW(model.parameters(), lr=0.0003, eps=1e-7, weight_decay=0.01)
criterion = CrossEntropyLoss()
num_epochs = 3
early_stopping_patience = 10
best_eval_loss = float("inf")
patience_counter = 0

label_map = {"entailed": 1, "not entailed": 0}

for epoch in range(num_epochs):

    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        print("the labels are", batch['labels'])
        
        mapped_label = [label_map[label] for label in batch['labels']]
        print("mapped label", mapped_label)

        labels = torch.tensor(mapped_label, dtype=torch.long).to(device) #change to tensor of long
        print("were at batch", batch)
        optimizer.zero_grad()
        outputs = model(input_ids)
        print("logits shape", outputs.shape)
        print("labels shape", outputs.shape)

        loss = criterion(outputs, labels.long())  # Ensure labels are correctly processed for CrossEntropyLoss
        loss.backward()
        optimizer.step()
        
        # Evaluate the model on the validation set
        model.eval()
        eval_loss = 0.0
        with torch.no_grad():
            for batch in eval_loader:
                input_texts, labels = batch
                tokenized_texts = [model.tokenizer.encode(text) for text in input_texts]
                max_seq_length = max(len(text) for text in tokenized_texts)
                padded_texts = []
                for text in tokenized_texts:
                    padded_text = text + [model.tokenizer.pad_token_id] * (max_seq_length - len(text))
                    padded_texts.append(padded_text)
                outputs = model(torch.tensor(padded_texts).to(device))
                loss = criterion(outputs.logit, torch.tensor(labels).to(device))
                eval_loss += loss.item()
        eval_loss /= len(eval_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Eval Loss: {eval_loss:.4f}")
        
        # Early stopping check
        if eval_loss < best_eval_loss:
            best_eval_loss = eval_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print("Early stopping triggered.")
                break
        
        model.train()

# Save the fine-tuned model
model.save_pretrained("finetuned_gpt2_logiqa")




# import sys
# import os
# import json
# from typing import List, Dict
# from transformer_lens import HookedTransformer
# import torch
# from torch.utils.data import Dataset, DataLoader, random_split
# from torch.nn.utils.rnn import pad_sequence


# sys.path.append("/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI")
# os.chdir("/Users/crayhippo/deduct/LogiQA2.0/logiqa2nli/data/QA2NLI")

# # Load the LogiQA2.0 train dataset
# with open("train.txt", "r") as file:
#     train_data = [json.loads(line) for line in file]

# # Remove indices that are too long
# train_data = [item for i, item in enumerate(train_data)]

# # Load the pre-trained GPT-2 model
# model = HookedTransformer.from_pretrained("gpt2")

# # Prepare the dataset for training
# def prepare_data(data: List[Dict]) -> List[Dict]:
#     prepared_data = []
#     for item in data:
#         label = item["label"]
#         major_premise = " ".join(item["major_premise"])
#         conclusion = item["conclusion"]
#         minor_premise = item["minor_premise"]
        
#         # Concatenate major premise, minor premise, and conclusion
#         input_text = f"Major Premise: {major_premise}\nMinor Premise: {minor_premise}\nConclusion: {conclusion}"
        
#         prepared_data.append({"input_text": input_text, "label": label})
    
#     return prepared_data

# train_dataset = prepare_data(train_data)


# # Create a custom dataset class
# class LogiQADataset(Dataset):
#     def __init__(self, data):
#         self.data = data
    
#     def __len__(self):
#         return len(self.data)
    
#     def __getitem__(self, idx):
#         item = self.data[idx]
#         input_text = item["input_text"]
#         label = item["label"]
#         return input_text, label

# # Create a DataLoader
# train_dataset = LogiQADataset(train_dataset)
# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)

# # Load the pre-trained GPT-2 model
# model = HookedTransformer.from_pretrained("gpt2")





# # Fine-tune the model
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model.to(device)
# model.train()

# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
# num_epochs = 3

# for epoch in range(num_epochs):
#     for batch in train_loader:
#         input_texts, labels = batch
        
#         # Tokenize the input texts
#         tokenized_texts = [model.tokenizer.encode(text) for text in input_texts]
#         print("we're at epoch", epoch, "batch", batch)
#         # Get the maximum sequence length in the current batch
#         max_seq_length = max(len(text) for text in tokenized_texts)
        
#         # Pad the tokenized texts to the maximum sequence length
#         padded_texts = []
#         for text in tokenized_texts:
#             padded_text = text + [model.tokenizer.pad_token_id] * (max_seq_length - len(text))
#             padded_texts.append(padded_text)
        
#         criterion = nn.CrossEntropyLoss()
#         optimizer.zero_grad()
#         loss = model(torch.tensor(padded_texts).to(device), labels)
#         loss.backward()
#         optimizer.step()

# # Save the fine-tuned model
# model.save_pretrained("finetuned_gpt2_logiqa")